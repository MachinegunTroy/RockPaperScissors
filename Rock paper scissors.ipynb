{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2016638-1566-4ff5-a6fb-a798955f495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, Tuple\n",
    "from gymnasium.utils import seeding\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e02da48-15f1-4841-b1df-49e5841f462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockPaperScissorsEnv(AECEnv):\n",
    "    metadata = {\"name\":\"RockPaperScissorsEnv_v0\"}\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.possible_agents = [\"player_0\",\"player_1\"]\n",
    "        self.agent_name_mapping = dict(zip(self.possible_agents,list(range(len(self.possible_agents)))))\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents} #0 for Rock, 1 for Paper, 2 for Scissors\n",
    "        self.observation_spaces = {agent: Discrete(4) for agent in self.possible_agents} #0-2 for Opponent's last move and 3 if he made the first move to mask his move\n",
    "        self.agents_moves = {}\n",
    "    \n",
    "    def observe(self,agent): #Observe opponents last move but return 3 instead if it is the first move\n",
    "        opponent = self.possible_agents[1-self.agent_name_mapping[agent]]\n",
    "        return self.agents_moves.get(opponent,3)\n",
    "\n",
    "    def reset(self,seed=None,options=None):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent:0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent:0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent:{} for agent in self.agents}\n",
    "\n",
    "        self.agents_moves.clear()\n",
    "\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "        observation = self.observe(self.agent_selection)\n",
    "        return observation, self.infos[self.agent_selection]\n",
    "\n",
    "    def step(self,action):\n",
    "        current_agent = self.agent_selection\n",
    "        self.agents_moves[current_agent] = action\n",
    "\n",
    "        if self._agent_selector.is_last(): #Resolve round if last agent has moved\n",
    "            if self.agents_moves[\"player_0\"] == self.agents_moves[\"player_1\"]: #Tie \n",
    "                self.rewards[\"player_0\"] = 0\n",
    "                self.rewards[\"player_1\"] = 0\n",
    "            elif self.agents_moves[\"player_0\"] == 0 and self.agents_moves[\"player_1\"] == 1:#If player_0 picks rock and player_1 picks paper\n",
    "                self.rewards[\"player_0\"] = -1\n",
    "                self.rewards[\"player_1\"] = 1\n",
    "            elif self.agents_moves[\"player_0\"] == 1 and self.agents_moves[\"player_1\"] == 2:#If player_0 picks paper and player_1 picks scissors\n",
    "                self.rewards[\"player_0\"] = -1\n",
    "                self.rewards[\"player_1\"] = 1\n",
    "            elif self.agents_moves[\"player_0\"] == 2 and self.agents_moves[\"player_1\"] == 0:#If player_0 picks scissors and player_1 picks rock\n",
    "                self.rewards[\"player_0\"] = -1\n",
    "                self.rewards[\"player_1\"] = 1\n",
    "            else: #If no win conditions for player_0 and not tie, player_1 wins\n",
    "                self.rewards[\"player_0\"] = 1\n",
    "                self.rewards[\"player_1\"] = -1\n",
    "                \n",
    "            self.terminations = {a: True for a in self.agents}\n",
    "            \n",
    "        self.agent_selection = self._agent_selector.next() #Switch to next agent\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "    def action_space(self,agent):\n",
    "        return self._action_spaces[agent]\n",
    "\n",
    "    def observation_space(self,agent):\n",
    "        return self._observation_spaces[agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b753c0-f851-45f4-abd9-ff26253fd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "\n",
    "NUM_EPISODES = 20000\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee056a2f-b30b-4702-a85b-72c86f9b286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(QNetwork,self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,output_size)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.action_size = output_size\n",
    "        self.observation_size = input_size\n",
    "        self.q_network = QNetwork(input_size=input_size,output_size=output_size) \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(),lr=LEARNING_RATE)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.epsilon = EPSILON_START\n",
    "    def choose_action(self,state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0,self.action_size-1) #Choose random action\n",
    "        else: \n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.LongTensor([state])\n",
    "                state_one_hot = F.one_hot(state_tensor,num_classes = self.observation_size).float()\n",
    "                q_values = self.q_network(state_one_hot)\n",
    "                return torch.argmax(q_values).item()\n",
    "                \n",
    "    def learn(self,state,action,reward,next_state,done):\n",
    "        state_tensor = torch.LongTensor([state])\n",
    "        state_one_hot = F.one_hot(state_tensor,num_classes = self.observation_size).float()\n",
    "        next_state_tensor = torch.LongTensor([next_state])\n",
    "        next_state_one_hot = F.one_hot(next_state_tensor,num_classes = self.observation_size).float()\n",
    "        action_tensor = torch.LongTensor([action])\n",
    "        reward_tensor = torch.FloatTensor([reward])\n",
    "        done_tensor = torch.BoolTensor([done])\n",
    "\n",
    "        current_q_values = self.q_network(state_one_hot)\n",
    "        current_q_for_action = current_q_values.gather(1,action_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.q_network(next_state_one_hot)\n",
    "            max_next_q = next_q_values.max(1)[0]\n",
    "            target_q = reward_tensor + (~done_tensor) * GAMMA * max_next_q\n",
    "\n",
    "        loss = self.loss_fn(current_q_for_action,target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(EPSILON_END,self.epsilon*EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc0f2a81-4b2d-4955-9775-a6ee3a52bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m termination \u001b[38;5;129;01mor\u001b[39;00m truncation:\u001b[38;5;66;03m# If agent is done, learn from its final experience\u001b[39;00m\n\u001b[32m     13\u001b[39m     exp = experience_buffer[agent_id]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     agents[agent_id].learn(exp[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m],exp[\u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m],reward,observation,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     16\u001b[39m current_agent = agents[agent_id]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mDQNAgent.learn\u001b[39m\u001b[34m(self, state, action, reward, next_state, done)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m,state,action,reward,next_state,done):\n\u001b[32m     33\u001b[39m     state_tensor = torch.LongTensor([state])\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     state_one_hot = F.one_hot(state_tensor,num_classes = \u001b[38;5;28mself\u001b[39m.observation_size).float()\n\u001b[32m     35\u001b[39m     next_state_tensor = torch.LongTensor([next_state])\n\u001b[32m     36\u001b[39m     next_state_one_hot = F.one_hot(next_state_tensor,num_classes = \u001b[38;5;28mself\u001b[39m.observation_size).float()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RockPaperScissorsEnv()\n",
    "    agents = {agent_id: DQNAgent(input_size=4,output_size=3) for agent_id in env.possible_agents} #0-2 for Rock,paper,scissors opponent last move,3 for opponent current move\n",
    "    total_rewards = {agent_id: 0 for agent_id in env.possible_agents}\n",
    "    print(\"Start training\")\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        observation,info = env.reset()\n",
    "        experience_buffer = {agent_id:{} for agent_id in env.possible_agents}#Store experiences for agents\n",
    "        \n",
    "        for agent_id in env.agent_iter():\n",
    "            observation,reward,termination,truncation,info = env.last()\n",
    "            if termination or truncation:# If agent is done, learn from its final experience\n",
    "                exp = experience_buffer[agent_id]\n",
    "                agents[agent_id].learn(exp['state'],exp['action'],reward,observation,True)\n",
    "                continue\n",
    "            current_agent = agents[agent_id]\n",
    "            action = current_agent.choose_action(observation)\n",
    "            experience_buffer[agent_id] = {\"state\":observation,\"action\":action}\n",
    "            env.step(action)\n",
    "\n",
    "        for agent in agents.values():\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        for agent_id in env.possible_agents:\n",
    "            total_rewards[agent_id] += env.rewards[agent_id]\n",
    "                \n",
    "        if (episode+1)%1000 == 0:\n",
    "            avg_reward_p0 = total_rewards['player_0']/1000\n",
    "            avg_reward_p1 = total_rewards['player_1']/1000\n",
    "            \n",
    "            print(f\"Episode: {episode+1}/{NUM_EPISODES} - Average Reward P0 (last 1k): {avg_reward_p0} - Average Reward P1 (last 1k): {avg_reward_p1}\")\n",
    "            print(f\"Episode: {episode+1}/{NUM_EPISODES} - Epsilon P0: {agents['player_0'].epsilon} - Epsilon P1: {agents['player_1'].epsilon}\")\n",
    "            print(f\"Last Observation:{observation}, info:{info}\")\n",
    "            total_rewards = {agent_id:0 for agent_id in env.possible_agents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baa1be-fd62-4e67-8db5-9ee0a8911b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7d8ba-5e92-4aed-825e-474650ea6c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc32f2-401f-4450-8e57-febb5cba99e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9ba2d-403f-443e-a0fd-01b3401bb4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81250819-03a8-48a3-afe6-6b57ac490951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee08c4c-1e51-4bdd-a90d-a49f3d97cd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ff175-951a-405f-977c-5a74ce3b812e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
